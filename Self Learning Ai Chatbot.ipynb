{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c628281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e5e81",
   "metadata": {},
   "source": [
    "Reading the Corpus of Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa958045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\EK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\EK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\EK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "f = open('C:/Users/EK/Desktop/imageclassification/AI chatbot/chatbot_info.txt', 'r', errors = 'ignore')\n",
    "raw_doc = f.read()\n",
    "raw_doc = raw_doc.lower() #Converting entire text to lowercase\n",
    "nltk.download('punkt') #Using the Punkt tokenizer\n",
    "nltk.download ('wordnet') #Using the wordnet dictionary\n",
    "nltk.download ('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbce7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = nltk.sent_tokenize(raw_doc)\n",
    "word_tokens = nltk.word_tokenize(raw_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edcb9c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the reason given is: this article is using citations from 1970 and virtually all claims about conversational capabilities are at least ten years out of date (for example the turing test was arguably made obsolete years ago by transformer models).'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99fa878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipediathe'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba521f8a",
   "metadata": {},
   "source": [
    "Performing Text-PreProcessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e77264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens (tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punc_dict = dict((ord (punct), None) for punct in string.punctuation)\n",
    "def LemNormalize (text):\n",
    "        return LemTokens (nltk.word_tokenize(text.lower().translate (remove_punc_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf534e0",
   "metadata": {},
   "source": [
    "Define Greeting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da404fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "greet_inputs = ('hello', 'hi', 'whassup', 'how are you?')\n",
    "greet_responses = ('hi', 'Hey', 'Hey There!', 'There there!!')\n",
    "def greet (sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greet_inputs:\n",
    "            return random.choice(greet_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6fac6a",
   "metadata": {},
   "source": [
    "Response Generation by the Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec253bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8b850e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response (user_response):\n",
    "    robo1_response =''\n",
    "    TfidfVec = TfidfVectorizer (tokenizer = LemNormalize, stop_words = 'english')\n",
    "    tfidf = TfidfVec.fit_transform (sentence_tokens)\n",
    "    vals = cosine_similarity (tfidf[-1], tfidf)\n",
    "    idx = vals.argsort ()[0][-2]\n",
    "    flat = vals. flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat [-2]\n",
    "    if (req_tfidf == 0):\n",
    "        robo1_response = robo1_response + \"I am sorry. Unable to understand you!\"\n",
    "        return robo1_response\n",
    "    else:\n",
    "        robo1_response = robo1_response+ sentence_tokens[idx]\n",
    "        return robo1_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebb9c2e",
   "metadata": {},
   "source": [
    "Defining the ChatFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de82ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = True\n",
    "print('Hello! I am the Learning Bot. Start typing your text after greeting to talk to me. For ending convo type bye!')\n",
    "while (flag == True):\n",
    "    user_response = input()\n",
    "    user_response = user_response. lower()\n",
    "    if(user_response != 'bye'):\n",
    "        if(user_response == 'thank you' or user_response == 'thanks'):\n",
    "            flag = False\n",
    "            print('Bot: You are welcome..')\n",
    "        else:\n",
    "            if(greet(user_response) != None):\n",
    "                print('Bot: ' + greet (user_response ))\n",
    "            else:\n",
    "                sentence_tokens. append(user_response)\n",
    "                word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
    "                final_words = list (set(word_tokens))\n",
    "                print('Bot: ', end = '')\n",
    "                print (response (user_response ))\n",
    "                sentence_tokens.remove(user_response)\n",
    "    else:\n",
    "          flag =False\n",
    "          print('Bot: Goodbye!')               \n",
    "                      \n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f6b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bias variance trade off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6279dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da66f7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
